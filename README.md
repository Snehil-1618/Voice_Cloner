# Voice_Cloner

Voice_Cloner is a text-to-speech program built on the under-lying Tortoise_TTS package with the added funcationalities like self-processing of audio files and text genetration for multiple target speakers at once additionally it prioritie:

1. Strong multi-voice capabilities.
2. Highly realistic prosody and intonation.


### Local Installation

If you want to use this on your own computer, you must have an NVIDIA GPU.

First, install miniconda: https://docs.conda.io/en/latest/miniconda.html

Then run the following commands, using anaconda prompt as the terminal (or any other terminal configured to work with conda)

This will:
1. create conda environment with minimal dependencies specified
2. activate the environment
3. install pytorch with the command provided here: https://pytorch.org/get-started/locally/ (Library for Pre_Trained Models)
4. clone tortoise-tts
5. change the current directory to tortoise-tts
6. install librosa,soundfile for additional preprocessing
7. run tortoise python setup install script

```shell
conda create --name tortoise python=3.9 numba inflect
conda activate tort2
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
conda install transformers=4.29.2
git clone https://github.com/neonbjb/tortoise-tts.git
conda install -c conda-forge pysoundfile
conda install -c conda-forge librosa
pip install -r requirements.txt
cd tortoise-tts
python setup.py install
```

### Adding a new voice

To add new voices to Tortoise, you will need to do the following:

1. Gather audio clips of your speaker(s).
2. Cut your clips into ~10 second segments. You want at least 3 clips. More is better.
3. Save your clips in the directory named <custom_voice_samples>
4. Create a sub_directory in the <tortoise/voices/{TARGET_SPEAKER}>.
4. Run the Rename.ipynb file to standardize the names of the file for training the model.
5. Run the Audio_Preprocess.ipynb to convert your input files to the standard input format for the model.
6. Run the 'tts.py' after setting the 'text' parameter as the text you want to process into the target speech.
7. The output file will be saved in the main 'tortoise_tts' directory with the name as <generate-{target_name}.wav>


# Rename.py

A simple Python script to batch rename WAV files in a folder numerically, specifically designed to facilitate the management and organization of audio files generated by the Voice_Clone Model.

## Features

- Automatically detects WAV files in a specified folder
- Supports both uppercase and lowercase file extensions (e.g., `.WAV` and `.wav`)
- Renames files sequentially with a numerical prefix (e.g., `1.wav`, `2.wav`, etc.)

## Usage

1. Update the `folder_path` variable in `Rename.ipynb` to point to the directory containing your WAV files.
2. Run Rename.ipynb using Python 3.9:


# Audio_PreProcessing.py

This file contains a Python script that preprocesses audio files for training the given Voice_Cloner model . The script trims silence, normalizes the audio, and saves the processed files to a specified output folder. It's specifically designed to work with .wav files to help create a clean and consistent dataset for the given model training.

## Requirements

- Python 3.6 or higher
- Librosa
- SoundFile


## Usage

1. Update the `input_path` and `output_path` variables in the `tacotron2_preprocessor.py` script to point to your input folder containing the .wav files and the desired output folder for the processed files.

input_path = "path\\to\\your\\input_folder"
output_path = "path\\to\\your\\output_folder"
Run the Audio_PreProcessing.ipynb


# tts.py

A simple Python script to execute the speech generation based on the voice fed to the model, specifically designed to facilitate the management and organization of audio files generated by the Voice_Clone Model.

## Features

- Uses the TextToSpeech API from tortoise and initiates the encoders and decoders of the given model
- Generates the speech for given text in different qualities based on the presets.
- Saves the generated files in the .wav format in the main directory

## Usage

1. Create text to speech based on the input voice samples.
2  Enables for efficient and faster generation using presets like 'ultra_fast'.
3. Edit the text parameter in the tts.py with the target text to be generated:
4. Edit the "Custom_Voice_Name"=<Name_of_the_target_speaker_directory>
5. Execute "tts.py" or run python tts.py
6. Output files will be generated in the "tortoise_tts" directory.



### API

Tortoise can be used programmatically, like so:

```python
reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]
tts = api.TextToSpeech()
pcm_audio = tts.tts_with_preset("your text here", voice_samples=reference_clips, preset='fast')
```


## Model architecture

Tortoise TTS is inspired by OpenAI's DALLE, applied to speech data and using a better decoder. It is made up of 5 separate
models that work together. I've assembled a write-up of the system architecture here:
[https://nonint.com/2022/04/25/tortoise-architectural-design-doc/](https://nonint.com/2022/04/25/tortoise-architectural-design-doc/)

## Training

These models were trained on my "homelab" server with 8 RTX 3090s over the course of several months. They were trained on a dataset consisting of
~50k hours of speech data, most of which was transcribed by [ocotillo](http://www.github.com/neonbjb/ocotillo). Training was done on my own
[DLAS](https://github.com/neonbjb/DL-Art-School) trainer.



## Acknowledgements

This project has garnered more praise than I expected. I am standing on the shoulders of giants, though, and I want to
credit a few of the amazing folks in the community that have helped make this happen:

- Hugging Face, who wrote the GPT model and the generate API used by Tortoise, and who hosts the model weights.
- [Ramesh et al](https://arxiv.org/pdf/2102.12092.pdf) who authored the DALLE paper, which is the inspiration behind Tortoise.
- [Nichol and Dhariwal](https://arxiv.org/pdf/2102.09672.pdf) who authored the (revision of) the code that drives the diffusion model.
- [Jang et al](https://arxiv.org/pdf/2106.07889.pdf) who developed and open-sourced univnet, the vocoder this repo uses.
- [Kim and Jung](https://github.com/mindslab-ai/univnet) who implemented univnet pytorch model.
- [lucidrains](https://github.com/lucidrains) who writes awesome open source pytorch models, many of which are used here.
- [Patrick von Platen](https://huggingface.co/patrickvonplaten) whose guides on setting up wav2vec were invaluable to building my dataset.

